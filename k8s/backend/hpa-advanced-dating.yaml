# Advanced Horizontal Pod Autoscaler for Dating Profile Optimizer Backend
# Optimized for dating app workload patterns with multiple metrics
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: dating-optimizer-backend-hpa
  namespace: dating-optimizer
  labels:
    app: dating-optimizer-backend
    component: autoscaling
    version: v1
  annotations:
    description: "Advanced HPA for dating app backend with multiple metrics"
    autoscaling.alpha.kubernetes.io/conditions: '[{"type":"AbleToScale","status":"True","reason":"SucceededRescale"}]'
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: dating-optimizer-backend
  
  # Dating app specific scaling configuration
  minReplicas: 3  # Always maintain minimum for availability
  maxReplicas: 50  # Allow significant scaling for peak times
  
  # Multiple metrics for intelligent scaling
  metrics:
  # CPU utilization - primary metric
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Scale at 70% CPU to handle spikes
  
  # Memory utilization - important for dating apps with image processing
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 75  # Scale at 75% memory usage
  
  # Custom metric: Active users (requests per second)
  - type: Pods
    pods:
      metric:
        name: dating_app_active_users_per_pod
      target:
        type: AverageValue
        averageValue: "50"  # 50 active users per pod
  
  # Custom metric: AI processing queue length
  - type: Object
    object:
      metric:
        name: dating_app_ai_queue_length
      describedObject:
        apiVersion: v1
        kind: Service
        name: dating-optimizer-backend
      target:
        type: Value
        value: "100"  # Scale when queue has 100+ items
  
  # Custom metric: Bio generation latency
  - type: Pods
    pods:
      metric:
        name: dating_app_bio_generation_latency_p95
      target:
        type: AverageValue
        averageValue: "8000"  # Scale when P95 latency > 8s
  
  # Custom metric: Photo analysis latency
  - type: Pods
    pods:
      metric:
        name: dating_app_photo_analysis_latency_p95
      target:
        type: AverageValue
        averageValue: "12000"  # Scale when P95 latency > 12s
  
  # External metric: Database connection pool usage
  - type: External
    external:
      metric:
        name: postgres_connection_pool_usage_percent
        selector:
          matchLabels:
            app: dating-optimizer-db
      target:
        type: Value
        value: "80"  # Scale when DB connections > 80%
  
  # Dating app specific scaling behavior
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60  # Wait 1 minute before scaling up
      policies:
      # Aggressive scale up for sudden traffic spikes (evening peak times)
      - type: Percent
        value: 100  # Can double pods quickly
        periodSeconds: 60
      # Conservative scale up for sustained load
      - type: Pods
        value: 5  # Add max 5 pods at once
        periodSeconds: 60
      selectPolicy: Max  # Use the more aggressive policy
    
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 minutes before scaling down
      policies:
      # Gradual scale down to avoid thrashing
      - type: Percent
        value: 25  # Remove max 25% of pods
        periodSeconds: 180
      # Conservative pod removal
      - type: Pods
        value: 2  # Remove max 2 pods at once
        periodSeconds: 60
      selectPolicy: Min  # Use the more conservative policy

---
# Vertical Pod Autoscaler for resource optimization
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: dating-optimizer-backend-vpa
  namespace: dating-optimizer
  labels:
    app: dating-optimizer-backend
    component: autoscaling
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: dating-optimizer-backend
  
  updatePolicy:
    updateMode: "Auto"  # Automatically apply recommendations
    minReplicas: 3
  
  resourcePolicy:
    containerPolicies:
    - containerName: dating-optimizer-backend
      # Dating app specific resource limits
      minAllowed:
        cpu: 100m
        memory: 256Mi
      maxAllowed:
        cpu: 4
        memory: 8Gi
      # Controlled resource scaling
      controlledResources: ["cpu", "memory"]
      controlledValues: RequestsAndLimits

---
# Pod Disruption Budget for high availability during scaling
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: dating-optimizer-backend-pdb
  namespace: dating-optimizer
  labels:
    app: dating-optimizer-backend
    component: availability
spec:
  selector:
    matchLabels:
      app: dating-optimizer-backend
  # Ensure high availability during scaling operations
  minAvailable: 2  # Always keep at least 2 pods running
  # Alternative: maxUnavailable: 25%

---
# KEDA ScaledObject for advanced event-driven autoscaling
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: dating-optimizer-backend-keda
  namespace: dating-optimizer
  labels:
    app: dating-optimizer-backend
    component: event-driven-scaling
spec:
  scaleTargetRef:
    name: dating-optimizer-backend
  
  # Dating app specific scaling configuration
  minReplicaCount: 3
  maxReplicaCount: 100  # Higher max for KEDA to handle extreme spikes
  pollingInterval: 10  # Check metrics every 10 seconds
  cooldownPeriod: 180  # 3 minutes cooldown after scaling
  idleReplicaCount: 3  # Never scale below 3
  
  triggers:
  # Redis queue length for AI processing
  - type: redis
    metadata:
      address: "redis-service:6379"
      listName: "dating:ai:processing_queue"
      listLength: "10"  # Scale when queue has 10+ items per pod
      enableTLS: "false"
    authenticationRef:
      name: redis-auth
  
  # Prometheus metrics for dating app
  - type: prometheus
    metadata:
      serverAddress: "http://prometheus:9090"
      metricName: "dating_app_active_users_total"
      threshold: "200"  # Scale when total active users > 200
      query: "sum(dating_app_active_users)"
  
  # Dating app evening peak time scaling
  - type: cron
    metadata:
      timezone: "America/New_York"  # Adjust based on user base
      start: "19:00"  # Start evening peak scaling
      end: "23:00"   # End evening peak scaling
      desiredReplicas: "15"  # Higher replica count during peak
  
  # Weekend peak scaling
  - type: cron
    metadata:
      timezone: "America/New_York"
      start: "18:00"
      end: "24:00"
      desiredReplicas: "20"  # Even higher on weekends
      cron: "0 18 * * 6,7"  # Saturday and Sunday
  
  # Bio generation queue scaling
  - type: redis
    metadata:
      address: "redis-service:6379"
      listName: "dating:bio:generation_queue"
      listLength: "5"  # Scale when bio queue has 5+ items per pod
  
  # Photo analysis queue scaling
  - type: redis
    metadata:
      address: "redis-service:6379"
      listName: "dating:photo:analysis_queue"
      listLength: "3"  # Scale when photo queue has 3+ items per pod

---
# Service Monitor for Prometheus metrics collection
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: dating-optimizer-backend-metrics
  namespace: dating-optimizer
  labels:
    app: dating-optimizer-backend
    component: monitoring
spec:
  selector:
    matchLabels:
      app: dating-optimizer-backend
  endpoints:
  - port: metrics
    path: /metrics
    interval: 15s
    scrapeTimeout: 10s
  # Dating app specific metric labels
  namespaceSelector:
    matchNames:
    - dating-optimizer

---
# PrometheusRule for dating app specific alerting
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: dating-optimizer-backend-alerts
  namespace: dating-optimizer
  labels:
    app: dating-optimizer-backend
    component: alerting
spec:
  groups:
  - name: dating-optimizer-backend-scaling
    interval: 30s
    rules:
    
    # High CPU utilization alert
    - alert: DatingAppHighCPUUsage
      expr: rate(container_cpu_usage_seconds_total{pod=~"dating-optimizer-backend-.*"}[5m]) * 100 > 85
      for: 2m
      labels:
        severity: warning
        component: autoscaling
        app: dating-optimizer-backend
      annotations:
        summary: "High CPU usage detected in Dating Optimizer Backend"
        description: "CPU usage is {{ $value }}% for pod {{ $labels.pod }}"
    
    # High memory utilization alert
    - alert: DatingAppHighMemoryUsage
      expr: (container_memory_usage_bytes{pod=~"dating-optimizer-backend-.*"} / container_spec_memory_limit_bytes) * 100 > 90
      for: 2m
      labels:
        severity: warning
        component: autoscaling
        app: dating-optimizer-backend
      annotations:
        summary: "High memory usage detected in Dating Optimizer Backend"
        description: "Memory usage is {{ $value }}% for pod {{ $labels.pod }}"
    
    # AI processing queue backlog
    - alert: DatingAppAIQueueBacklog
      expr: dating_app_ai_queue_length > 200
      for: 1m
      labels:
        severity: critical
        component: ai-processing
        app: dating-optimizer-backend
      annotations:
        summary: "AI processing queue backlog detected"
        description: "AI queue has {{ $value }} pending items"
    
    # Bio generation latency alert
    - alert: DatingAppBioGenerationSlow
      expr: dating_app_bio_generation_latency_p95 > 15000
      for: 2m
      labels:
        severity: warning
        component: ai-processing
        feature: bio-generation
      annotations:
        summary: "Bio generation is slow"
        description: "Bio generation P95 latency is {{ $value }}ms"
    
    # Photo analysis latency alert
    - alert: DatingAppPhotoAnalysisSlow
      expr: dating_app_photo_analysis_latency_p95 > 20000
      for: 2m
      labels:
        severity: warning
        component: ai-processing
        feature: photo-analysis
      annotations:
        summary: "Photo analysis is slow"
        description: "Photo analysis P95 latency is {{ $value }}ms"
    
    # Scaling events monitoring
    - alert: DatingAppFrequentScaling
      expr: changes(kube_deployment_status_replicas{deployment="dating-optimizer-backend"}[10m]) > 5
      for: 1m
      labels:
        severity: info
        component: autoscaling
      annotations:
        summary: "Frequent scaling events detected"
        description: "Backend has scaled {{ $value }} times in the last 10 minutes"
    
    # Dating app specific business metrics
    - alert: DatingAppLowUserEngagement
      expr: dating_app_active_users_per_pod < 10
      for: 5m
      labels:
        severity: warning
        component: business-metrics
      annotations:
        summary: "Low user engagement detected"
        description: "Only {{ $value }} active users per pod"
    
    # Database connection pool warning
    - alert: DatingAppHighDBConnections
      expr: postgres_connection_pool_usage_percent > 85
      for: 1m
      labels:
        severity: warning
        component: database
      annotations:
        summary: "High database connection usage"
        description: "Database connection pool usage is {{ $value }}%"

---
# ConfigMap for autoscaling configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: dating-optimizer-autoscaling-config
  namespace: dating-optimizer
  labels:
    app: dating-optimizer-backend
    component: configuration
data:
  # Dating app peak hours configuration
  peak_hours.json: |
    {
      "weekday_peaks": [
        {"start": "07:00", "end": "09:00", "scale_factor": 1.5},
        {"start": "12:00", "end": "14:00", "scale_factor": 1.3},
        {"start": "19:00", "end": "23:00", "scale_factor": 2.0}
      ],
      "weekend_peaks": [
        {"start": "10:00", "end": "14:00", "scale_factor": 1.8},
        {"start": "18:00", "end": "24:00", "scale_factor": 2.5}
      ],
      "holiday_multipliers": {
        "valentines_day": 3.0,
        "new_years": 2.5,
        "summer_start": 1.5
      }
    }
  
  # Dating app scaling thresholds
  scaling_thresholds.yaml: |
    cpu:
      scale_up: 70
      scale_down: 30
    memory:
      scale_up: 75
      scale_down: 40
    active_users_per_pod:
      target: 50
      max: 100
    ai_processing:
      bio_generation_latency_ms: 8000
      photo_analysis_latency_ms: 12000
      queue_length_per_pod: 10
    database:
      connection_pool_percent: 80
      query_latency_ms: 500
  
  # Regional scaling configuration
  regional_config.yaml: |
    regions:
      us_east:
        peak_offset: "0h"
        base_replicas: 5
        max_replicas: 30
      us_west:
        peak_offset: "-3h"
        base_replicas: 3
        max_replicas: 20
      europe:
        peak_offset: "+6h"
        base_replicas: 2
        max_replicas: 15
      asia:
        peak_offset: "+12h"
        base_replicas: 2
        max_replicas: 10