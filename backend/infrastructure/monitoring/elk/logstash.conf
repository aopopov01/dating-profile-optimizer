# Logstash Configuration for Dating Profile Optimizer
# Comprehensive log processing pipeline for production monitoring

input {
  # Application logs from Dating Optimizer API
  beats {
    port => 5044
    type => "application"
  }
  
  # Nginx access logs
  file {
    path => "/nginx/logs/access.log"
    start_position => "beginning"
    type => "nginx-access"
    codec => json
  }
  
  # Nginx error logs
  file {
    path => "/nginx/logs/error.log"
    start_position => "beginning"
    type => "nginx-error"
    codec => multiline {
      pattern => "^\d{4}/\d{2}/\d{2}"
      negate => true
      what => "previous"
    }
  }
  
  # Application logs
  file {
    path => "/app/logs/combined.log"
    start_position => "beginning"
    type => "app-combined"
    codec => json
  }
  
  file {
    path => "/app/logs/error.log"
    start_position => "beginning"
    type => "app-error"
    codec => json
  }
  
  # PostgreSQL logs
  file {
    path => "/postgres/logs/postgresql.log"
    start_position => "beginning"
    type => "postgresql"
    codec => multiline {
      pattern => "^\d{4}-\d{2}-\d{2}"
      negate => true
      what => "previous"
    }
  }
  
  # Redis logs
  file {
    path => "/redis/logs/redis.log"
    start_position => "beginning"
    type => "redis"
  }
  
  # System logs via syslog
  syslog {
    port => 5140
    type => "syslog"
  }
}

filter {
  # Common fields for all logs
  mutate {
    add_field => {
      "environment" => "${ENVIRONMENT:production}"
      "application" => "dating-profile-optimizer"
      "cluster" => "${CLUSTER_NAME:production}"
    }
  }

  # Process application logs
  if [type] == "application" or [type] == "app-combined" or [type] == "app-error" {
    # Parse JSON structured logs
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
      }
    }
    
    # Extract request ID for tracing
    if [request_id] {
      mutate {
        add_field => { "trace_id" => "%{request_id}" }
      }
    }
    
    # Parse log levels
    if [level] {
      mutate {
        lowercase => [ "level" ]
      }
      
      if [level] == "error" or [level] == "fatal" {
        mutate {
          add_tag => [ "error" ]
          add_field => { "severity" => "high" }
        }
      } else if [level] == "warn" {
        mutate {
          add_tag => [ "warning" ]
          add_field => { "severity" => "medium" }
        }
      } else {
        mutate {
          add_field => { "severity" => "low" }
        }
      }
    }
    
    # Parse user information
    if [user_id] {
      mutate {
        add_field => { "user_context" => "%{user_id}" }
      }
    }
    
    # Identify AI-related logs
    if [message] =~ /(?i)(openai|ai|gpt|analysis|photo)/ {
      mutate {
        add_tag => [ "ai-processing" ]
      }
    }
    
    # Identify payment-related logs
    if [message] =~ /(?i)(payment|stripe|subscription|billing)/ {
      mutate {
        add_tag => [ "payment" ]
      }
    }
    
    # Identify authentication logs
    if [message] =~ /(?i)(login|logout|auth|register|password)/ {
      mutate {
        add_tag => [ "authentication" ]
      }
    }
  }

  # Process Nginx access logs
  if [type] == "nginx-access" {
    grok {
      match => {
        "message" => '%{IPORHOST:client_ip} - %{DATA:user_name} \[%{HTTPDATE:timestamp}\] "%{WORD:http_method} %{DATA:url} HTTP/%{NUMBER:http_version}" %{NUMBER:response_code} %{NUMBER:body_sent_bytes} "%{DATA:referrer}" "%{DATA:user_agent}" "%{DATA:forwarded_for}" rt=%{NUMBER:request_time} uct="%{DATA:upstream_connect_time}" uht="%{DATA:upstream_header_time}" urt="%{DATA:upstream_response_time}"'
      }
    }
    
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    
    # Convert numeric fields
    mutate {
      convert => {
        "response_code" => "integer"
        "body_sent_bytes" => "integer"
        "request_time" => "float"
        "upstream_response_time" => "float"
      }
    }
    
    # Classify response codes
    if [response_code] >= 500 {
      mutate {
        add_tag => [ "error", "server-error" ]
        add_field => { "severity" => "high" }
      }
    } else if [response_code] >= 400 {
      mutate {
        add_tag => [ "client-error" ]
        add_field => { "severity" => "medium" }
      }
    } else if [response_code] >= 300 {
      mutate {
        add_tag => [ "redirect" ]
        add_field => { "severity" => "low" }
      }
    } else {
      mutate {
        add_field => { "severity" => "low" }
      }
    }
    
    # Identify slow requests
    if [request_time] and [request_time] > 2.0 {
      mutate {
        add_tag => [ "slow-request" ]
      }
    }
    
    # Parse user agent for analytics
    useragent {
      source => "user_agent"
      target => "ua"
    }
    
    # GeoIP lookup for client IP
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }

  # Process Nginx error logs
  if [type] == "nginx-error" {
    grok {
      match => {
        "message" => '%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:log_level}\] %{POSINT:pid}#%{POSINT:tid}: %{GREEDYDATA:error_message}'
      }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
    }
    
    mutate {
      add_tag => [ "nginx-error" ]
      add_field => { "severity" => "medium" }
    }
  }

  # Process PostgreSQL logs
  if [type] == "postgresql" {
    grok {
      match => {
        "message" => '%{TIMESTAMP_ISO8601:timestamp} \[%{NUMBER:pid}\] %{WORD:log_level}:  %{GREEDYDATA:pg_message}'
      }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
    }
    
    # Identify slow queries
    if [pg_message] =~ /duration: (\d+\.\d+) ms/ {
      grok {
        match => { "pg_message" => "duration: %{NUMBER:query_duration} ms" }
      }
      
      mutate {
        convert => { "query_duration" => "float" }
      }
      
      if [query_duration] and [query_duration] > 1000 {
        mutate {
          add_tag => [ "slow-query" ]
        }
      }
    }
    
    # Identify connection issues
    if [pg_message] =~ /(?i)(connection|timeout|refused)/ {
      mutate {
        add_tag => [ "connection-issue" ]
        add_field => { "severity" => "high" }
      }
    }
    
    # Identify deadlocks
    if [pg_message] =~ /(?i)deadlock/ {
      mutate {
        add_tag => [ "deadlock" ]
        add_field => { "severity" => "high" }
      }
    }
  }

  # Process Redis logs
  if [type] == "redis" {
    grok {
      match => {
        "message" => '%{POSINT:pid}:%{CHAR:role} %{TIMESTAMP_ISO8601:timestamp} %{CHAR:log_level} %{GREEDYDATA:redis_message}'
      }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
    }
    
    # Identify memory issues
    if [redis_message] =~ /(?i)(memory|oom)/ {
      mutate {
        add_tag => [ "memory-issue" ]
        add_field => { "severity" => "high" }
      }
    }
  }

  # Security event detection across all log types
  if [message] =~ /(?i)(hack|attack|inject|exploit|malware|virus|threat)/ {
    mutate {
      add_tag => [ "security-threat" ]
      add_field => { "severity" => "critical" }
    }
  }
  
  # Detect suspicious IP patterns
  if [client_ip] and [client_ip] !~ /^(10\.|192\.168\.|172\.(1[6-9]|2[0-9]|3[01])\.)/ {
    # Multiple failed attempts from same IP
    if [response_code] == 401 or [response_code] == 403 {
      mutate {
        add_tag => [ "auth-failure" ]
      }
    }
  }
  
  # Performance monitoring tags
  if [request_time] {
    if [request_time] > 5.0 {
      mutate {
        add_tag => [ "very-slow-request" ]
        add_field => { "performance_issue" => "critical" }
      }
    } else if [request_time] > 2.0 {
      mutate {
        add_tag => [ "slow-request" ]
        add_field => { "performance_issue" => "warning" }
      }
    }
  }
  
  # Clean up fields
  mutate {
    remove_field => [ "beat", "offset", "prospector" ]
  }
  
  # Add final timestamp if not present
  if ![timestamp] {
    mutate {
      add_field => { "timestamp" => "%{@timestamp}" }
    }
  }
}

output {
  # Send to Elasticsearch with dynamic index names
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "dating-optimizer-%{type}-%{+YYYY.MM.dd}"
    template_name => "dating-optimizer"
    template => "/etc/logstash/templates/dating-optimizer-template.json"
    template_overwrite => true
    
    # Use document type for older ES versions
    document_type => "_doc"
  }
  
  # Send critical errors to dead letter queue for manual review
  if "error" in [tags] or "security-threat" in [tags] or [severity] == "critical" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "dating-optimizer-critical-%{+YYYY.MM.dd}"
    }
  }
  
  # Send performance issues to separate index
  if "slow-request" in [tags] or "very-slow-request" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "dating-optimizer-performance-%{+YYYY.MM.dd}"
    }
  }
  
  # Debug output (disabled in production)
  # stdout {
  #   codec => rubydebug
  # }
  
  # Send metrics to monitoring system
  if [type] == "application" {
    http {
      url => "http://prometheus-pushgateway:9091/metrics/job/logstash/instance/dating-optimizer"
      http_method => "post"
      content_type => "text/plain"
      mapping => {
        "logstash_messages_total" => "1"
        "logstash_errors_total" => "%{[tags]}"
      }
    }
  }
}

# Global settings
settings:
  pipeline.workers: 4
  pipeline.batch.size: 1000
  pipeline.batch.delay: 50