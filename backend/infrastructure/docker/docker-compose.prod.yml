version: '3.8'

# Production Docker Compose Configuration
# Dating Profile Optimizer - Optimized for AWS/DigitalOcean deployment
# Supports high availability, auto-scaling, and comprehensive monitoring

services:
  # Load Balancer & SSL Termination
  nginx:
    image: nginx:1.25-alpine
    container_name: dating-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./ssl/certbot:/etc/letsencrypt:ro
      - ./ssl/dhparam:/etc/ssl/certs:ro
      - nginx_logs:/var/log/nginx
    depends_on:
      - api
    networks:
      - dating-network
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
        max_attempts: 3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.nginx.rule=Host(`api.datingoptimizer.com`)"
      - "traefik.http.routers.nginx.tls=true"

  # Backend API - Multiple Instances for HA
  api:
    image: dating-optimizer-backend:latest
    restart: unless-stopped
    environment:
      - NODE_ENV=production
      - PORT=3000
      - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres-master:5432/${POSTGRES_DB}
      - DATABASE_READ_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres-replica:5432/${POSTGRES_DB}
      - REDIS_URL=redis://redis-cluster:6379
      - REDIS_CLUSTER_NODES=redis-node1:6379,redis-node2:6379,redis-node3:6379
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4}
      - STRIPE_SECRET_KEY=${STRIPE_SECRET_KEY}
      - STRIPE_WEBHOOK_SECRET=${STRIPE_WEBHOOK_SECRET}
      - CLOUDINARY_CLOUD_NAME=${CLOUDINARY_CLOUD_NAME}
      - CLOUDINARY_API_KEY=${CLOUDINARY_API_KEY}
      - CLOUDINARY_API_SECRET=${CLOUDINARY_API_SECRET}
      - MIXPANEL_TOKEN=${MIXPANEL_TOKEN}
      - JWT_SECRET=${JWT_SECRET}
      - RATE_LIMIT_WINDOW=900000
      - RATE_LIMIT_MAX=100
      - CORS_ORIGINS=${CORS_ORIGINS}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - PROMETHEUS_METRICS=true
      - JAEGER_ENDPOINT=${JAEGER_ENDPOINT}
    networks:
      - dating-network
    depends_on:
      postgres-master:
        condition: service_healthy
      redis-cluster:
        condition: service_healthy
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
        monitor: 60s
      restart_policy:
        condition: on-failure
        max_attempts: 3
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    volumes:
      - app_logs:/app/logs
    labels:
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=3000"
      - "prometheus.io/path=/metrics"

  # PostgreSQL Master Database
  postgres-master:
    image: postgres:15-alpine
    container_name: postgres-master
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_REPLICATION_USER=${POSTGRES_REPLICATION_USER}
      - POSTGRES_REPLICATION_PASSWORD=${POSTGRES_REPLICATION_PASSWORD}
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres_master_data:/var/lib/postgresql/data
      - ./postgres/postgresql-master.conf:/etc/postgresql/postgresql.conf:ro
      - ./postgres/pg_hba.conf:/etc/postgresql/pg_hba.conf:ro
      - ./postgres/init-master.sh:/docker-entrypoint-initdb.d/init-master.sh:ro
      - postgres_logs:/var/log/postgresql
    networks:
      - dating-network
    ports:
      - "5432:5432"
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        max_attempts: 3
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    command: >
      postgres
      -c config_file=/etc/postgresql/postgresql.conf
      -c hba_file=/etc/postgresql/pg_hba.conf
      -c log_destination=stderr
      -c log_statement=all
      -c log_min_duration_statement=1000

  # PostgreSQL Read Replica
  postgres-replica:
    image: postgres:15-alpine
    container_name: postgres-replica
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_MASTER_SERVICE=postgres-master
      - POSTGRES_REPLICATION_USER=${POSTGRES_REPLICATION_USER}
      - POSTGRES_REPLICATION_PASSWORD=${POSTGRES_REPLICATION_PASSWORD}
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres_replica_data:/var/lib/postgresql/data
      - ./postgres/postgresql-replica.conf:/etc/postgresql/postgresql.conf:ro
      - ./postgres/init-replica.sh:/docker-entrypoint-initdb.d/init-replica.sh:ro
    networks:
      - dating-network
    depends_on:
      postgres-master:
        condition: service_healthy
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        max_attempts: 3
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis Cluster for Caching
  redis-cluster:
    image: redis:7-alpine
    container_name: redis-master
    restart: unless-stopped
    command: >
      redis-server
      --appendonly yes
      --cluster-enabled yes
      --cluster-config-file nodes.conf
      --cluster-node-timeout 5000
      --dir /data
      --port 6379
    volumes:
      - redis_master_data:/data
      - redis_logs:/var/log/redis
    networks:
      - dating-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        max_attempts: 3
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis Node 1
  redis-node1:
    image: redis:7-alpine
    restart: unless-stopped
    command: >
      redis-server
      --appendonly yes
      --cluster-enabled yes
      --cluster-config-file nodes.conf
      --cluster-node-timeout 5000
      --dir /data
      --port 6379
    volumes:
      - redis_node1_data:/data
    networks:
      - dating-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  # Redis Node 2
  redis-node2:
    image: redis:7-alpine
    restart: unless-stopped
    command: >
      redis-server
      --appendonly yes
      --cluster-enabled yes
      --cluster-config-file nodes.conf
      --cluster-node-timeout 5000
      --dir /data
      --port 6379
    volumes:
      - redis_node2_data:/data
    networks:
      - dating-network
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure

  # Elasticsearch for Logging
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    container_name: elasticsearch
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
      - xpack.security.enabled=false
      - xpack.monitoring.collection.enabled=true
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
      - elasticsearch_logs:/usr/share/elasticsearch/logs
    networks:
      - dating-network
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Logstash
  logstash:
    image: docker.elastic.co/logstash/logstash:8.8.0
    container_name: logstash
    restart: unless-stopped
    volumes:
      - ./elk/logstash/config:/usr/share/logstash/config:ro
      - ./elk/logstash/pipeline:/usr/share/logstash/pipeline:ro
      - app_logs:/app/logs:ro
      - nginx_logs:/nginx/logs:ro
      - postgres_logs:/postgres/logs:ro
      - redis_logs:/redis/logs:ro
    networks:
      - dating-network
    depends_on:
      elasticsearch:
        condition: service_healthy
    ports:
      - "5044:5044"
      - "9600:9600"

  # Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    container_name: kibana
    restart: unless-stopped
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - SERVER_HOST=0.0.0.0
    ports:
      - "5601:5601"
    networks:
      - dating-network
    depends_on:
      elasticsearch:
        condition: service_healthy

  # Prometheus Monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=30d'
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    networks:
      - dating-network
    ports:
      - "9090:9090"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Grafana Dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
      - GF_SERVER_ROOT_URL=https://monitoring.datingoptimizer.com
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - dating-network
    ports:
      - "3001:3000"
    depends_on:
      - prometheus

  # Alert Manager
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    restart: unless-stopped
    volumes:
      - ./monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    networks:
      - dating-network
    ports:
      - "9093:9093"
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=https://alerts.datingoptimizer.com'

  # Node Exporter for System Metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - dating-network
    ports:
      - "9100:9100"

  # Jaeger Tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    restart: unless-stopped
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
    ports:
      - "16686:16686"
      - "14268:14268"
    networks:
      - dating-network

  # Backup Service
  backup:
    image: dating-optimizer-backup:latest
    container_name: backup-service
    restart: unless-stopped
    environment:
      - POSTGRES_HOST=postgres-master
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_S3_BUCKET=${AWS_S3_BUCKET}
      - BACKUP_SCHEDULE=${BACKUP_SCHEDULE:-0 2 * * *}
    volumes:
      - backup_data:/backups
      - ./backup/scripts:/backup-scripts:ro
    networks:
      - dating-network
    depends_on:
      postgres-master:
        condition: service_healthy

# Volumes for persistent data
volumes:
  postgres_master_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/dating-optimizer/data/postgres-master
  postgres_replica_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/dating-optimizer/data/postgres-replica
  redis_master_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/dating-optimizer/data/redis-master
  redis_node1_data:
    driver: local
  redis_node2_data:
    driver: local
  elasticsearch_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/dating-optimizer/data/elasticsearch
  prometheus_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/dating-optimizer/data/prometheus
  grafana_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/dating-optimizer/data/grafana
  alertmanager_data:
    driver: local
  backup_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /opt/dating-optimizer/backups
  app_logs:
    driver: local
  nginx_logs:
    driver: local
  postgres_logs:
    driver: local
  redis_logs:
    driver: local
  elasticsearch_logs:
    driver: local

# Networks
networks:
  dating-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16